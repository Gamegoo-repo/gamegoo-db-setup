import mysql.connector
import os
import boto3
import json
import csv
from dotenv import load_dotenv
from datetime import datetime
from modules import primary_key

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# í™˜ê²½ë³€ìˆ˜
AWS_ACCESS_KEY = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
AWS_REGION = os.getenv("AWS_DEFAULT_REGION", "ap-northeast-2")
DB_HOST = os.getenv("RDS_HOST")
DB_USER = os.getenv("RDS_USER")
DB_PASSWORD = os.getenv("RDS_PASSWORD")
DB_SCHEMA = os.getenv("RDS_DB")
S3_BUCKET = 'gamegoo'

# boto3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±
s3 = boto3.client(
    's3',
    aws_access_key_id=AWS_ACCESS_KEY,
    aws_secret_access_key=AWS_SECRET_KEY,
    region_name=AWS_REGION
)

# s3 ë²„í‚·ì— íŒŒì¼ ì—…ë¡œë“œ
def upload_to_s3(filepath, filename):
    S3_KEY=f"db_data/{filename}"
    s3.upload_file(filepath, S3_BUCKET, S3_KEY)
    print("âœ… s3 Upload complete.")

# í•´ë‹¹ í…Œì´ë¸”ì˜ ëª¨ë“  ì¸ë±ìŠ¤ ì¡°íšŒ
def get_index_definitions(cursor, table_name):
    cursor.execute(f"""
        SELECT INDEX_NAME, COLUMN_NAME, SEQ_IN_INDEX, NON_UNIQUE
        FROM information_schema.STATISTICS
        WHERE TABLE_SCHEMA = %s AND TABLE_NAME = %s
        ORDER BY INDEX_NAME, SEQ_IN_INDEX;
    """, (DB_SCHEMA, table_name))

    indexes = {}
    for index_name, column_name, seq, non_unique in cursor.fetchall():
        if index_name not in indexes:
            indexes[index_name] = {'columns': [], 'non_unique': non_unique}
        indexes[index_name]['columns'].append(column_name)
    print(f"â˜‘ï¸ ì¸ë±ìŠ¤ ì¡°íšŒ ì™„ë£Œ: {table_name}")
    return indexes

# DROP ëŒ€ìƒ ì¸ë±ìŠ¤ ì¡°íšŒ
def get_safe_indexes_to_drop(cursor, table_name):
    safe_indexes={}
    try:
        full_indexes = {}
        indexes_to_exclude = set()
        auto_increment_column = {}
        
        if table_name in primary_key.DICT:
            auto_increment_column = primary_key.DICT[table_name]

        # ğŸ”¹ ì™¸ë˜í‚¤ ì œì•½ì¡°ê±´ì— ì‚¬ìš©ë˜ëŠ” ì¸ë±ìŠ¤ ì¡°íšŒ ë° ì œì™¸ ì²˜ë¦¬
        cursor.execute("""
            SELECT DISTINCT s.INDEX_NAME
            FROM information_schema.KEY_COLUMN_USAGE k
            JOIN information_schema.STATISTICS s
            ON k.TABLE_SCHEMA = s.TABLE_SCHEMA
            AND k.TABLE_NAME = s.TABLE_NAME
            AND k.COLUMN_NAME = s.COLUMN_NAME
            WHERE k.TABLE_SCHEMA = %s
                AND k.TABLE_NAME = %s
                AND k.REFERENCED_TABLE_NAME IS NOT NULL;
        """, (DB_SCHEMA, table_name))
        fk_index_names = {row[0] for row in cursor.fetchall()}
        indexes_to_exclude.update(fk_index_names)

        cursor.execute("""
            SELECT
                s.INDEX_NAME,
                s.COLUMN_NAME,
                s.SEQ_IN_INDEX,
                s.NON_UNIQUE,
                s.INDEX_TYPE,
                s.COMMENT
            FROM information_schema.STATISTICS s
            WHERE s.TABLE_SCHEMA = %s
            AND s.TABLE_NAME = %s
            ORDER BY s.INDEX_NAME, s.SEQ_IN_INDEX;
        """, (DB_SCHEMA, table_name))

        # ì¸ë±ìŠ¤ ì „ì²´ êµ¬ì„± ë° auto_increment ì»¬ëŸ¼ ê´€ë ¨ ì¸ë±ìŠ¤ ì¶”ì¶œ
        for index_name, column_name, seq, non_unique, index_type, comment in cursor.fetchall():
            if index_name not in full_indexes:
                full_indexes[index_name] = {
                    'columns': [], # (col_name, seq)
                    'non_unique': non_unique,
                    'index_type': index_type,
                    'comment': comment
                }
            full_indexes[index_name]['columns'].append((column_name, seq))

            if column_name == auto_increment_column:
                indexes_to_exclude.add(index_name)

        # ì •ë ¬ í›„ column ì´ë¦„ë§Œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        # ë³µí•© ì¸ë±ìŠ¤ column ìˆœì„œ ë°˜ì˜
        for idx_info in full_indexes.values():
            idx_info['columns'] = [
                col for col, _ in sorted(idx_info['columns'], key=lambda x: x[1])
            ]

        # PRIMARYì™€ auto_increment, FK í¬í•¨ ì¸ë±ìŠ¤ ì œì™¸
        safe_indexes = {
            name: info
            for name, info in full_indexes.items()
            if name != 'PRIMARY' and name not in indexes_to_exclude
        }
    except Exception as error:
        print(f"âŒ DROP ê°€ëŠ¥í•œ ì¸ë±ìŠ¤ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}")

    print(f"â˜‘ï¸ DROP ê°€ëŠ¥í•œ ì¸ë±ìŠ¤ ê°œìˆ˜: {len(safe_indexes)}")
    return safe_indexes

# ì¸ë±ìŠ¤ ë°±ì—… íŒŒì¼ ìƒì„±
def save_index_backup(indexes, table_name):
    try:
        timestamp = datetime.now().strftime("%m%d_%H%M%S")
        FILE_NAME = f"./index_backup/{table_name}_index_backup_{timestamp}.json"
        with open(FILE_NAME, 'w') as f:
            json.dump(indexes, f, indent=2)
    except Exception as error:
        print(f"âŒ ì¸ë±ìŠ¤ ë°±ì—… íŒŒì¼ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error}")
    
    print(f"â˜‘ï¸ ì¸ë±ìŠ¤ ë°±ì—… ì €ì¥: {FILE_NAME}")
    return FILE_NAME

# í•´ë‹¹ í…Œì´ë¸”ì˜ ëª¨ë“  ì¸ë±ìŠ¤ ì œê±°
def drop_indexes(cursor, table_name, indexes):
    for index_name in indexes:
        if index_name == 'PRIMARY':
            cursor.execute(f"ALTER TABLE `{table_name}` DROP PRIMARY KEY;")
        else:
            cursor.execute(f"ALTER TABLE `{table_name}` DROP INDEX `{index_name}`;")
    print("â˜‘ï¸ ì œê±° ëŒ€ìƒ ì¸ë±ìŠ¤ ì œê±° ì™„ë£Œ")

# í•´ë‹¹ í…Œì´ë¸”ì˜ ëª¨ë“  ì¸ë±ìŠ¤ ë³µêµ¬
def recreate_indexes(cursor, table_name, indexes):
    for name, info in indexes.items():
        cols = ', '.join(info['columns'])
        if name == 'PRIMARY':
            cursor.execute(f"ALTER TABLE `{table_name}` ADD PRIMARY KEY ({cols});")
        else:
            unique = 'UNIQUE ' if info['non_unique'] == 0 else ''
            cursor.execute(f"ALTER TABLE `{table_name}` ADD {unique}INDEX `{name}` ({cols});")
    print("â˜‘ï¸ ì¸ë±ìŠ¤ ë³µêµ¬ ì™„ë£Œ")

# rdsì— ë°ì´í„° ë¡œë“œ
def load_data_local_infile(cursor,filepath,table_name):
    columns_str = ""
    # 1. í—¤ë” ì¶”ì¶œ
    with open(filepath, newline='') as f:
        reader = csv.reader(f)
        headers = next(reader)  # ì²« ì¤„ ì½ê¸°

        # 2. ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´ ìƒì„±
        columns_str = ', '.join([f'`{col.strip()}`' for col in headers])

    print(f"ğŸ“¥ LOAD DATA LOCAL INFILE ì‹¤í–‰ ì¤‘...")
    sql = f"""
        LOAD DATA LOCAL INFILE '{filepath}'
        INTO TABLE {table_name}
        FIELDS TERMINATED BY ',' ENCLOSED BY '"'
        LINES TERMINATED BY '\\n'
        IGNORE 1 LINES
        ({columns_str});
    """

    cursor.execute(sql)


# rdsì— csv íŒŒì¼ LOAD DATA
def load_csv_with_local_infile(filepath, table_name):
    ABS_PATH = os.path.abspath(filepath)
    try:
        conn = mysql.connector.connect(
            host=DB_HOST,
            user=DB_USER,
            password=DB_PASSWORD,
            database=DB_SCHEMA,
            allow_local_infile=True  
        )
    except mysql.connector.Error as conn_err:
        print(f"âŒ RDS ì—°ê²° ì‹¤íŒ¨: {conn_err}")
        return
    
    cursor = conn.cursor()
    indexes = {}
    inserted_count = 0
    backup_file_name=""

    try:
        # ì‚½ì… ì „ row ìˆ˜
        cursor.execute(f"SELECT COUNT(*) FROM `{table_name}`")
        before_count = cursor.fetchone()[0]

        
        cursor.execute("SET FOREIGN_KEY_CHECKS = 0;")
        print("â˜‘ï¸ ì™¸ë˜í‚¤ ì œì•½ ì¡°ê±´ ë¹„í™œì„±í™”")

        indexes = get_safe_indexes_to_drop(cursor, table_name)
        if indexes:
            backup_file_name = save_index_backup(indexes, table_name)
            drop_indexes(cursor, table_name, indexes)
            conn.commit()

        # rdsì— ë°ì´í„° ë¡œë“œ
        load_data_local_infile(cursor,ABS_PATH,table_name)
        conn.commit()

        # ì‚½ì… í›„ row ìˆ˜
        cursor.execute(f"SELECT COUNT(*) FROM `{table_name}`")
        after_count = cursor.fetchone()[0]
        inserted_count = after_count - before_count
    
    except mysql.connector.Error as e:
        print(f"âŒ ì‚½ì… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print("ğŸ›  ì¸ë±ìŠ¤ ë³µêµ¬ ì‹œë„ ì¤‘...")
        try:
            recreate_indexes(cursor, table_name, indexes)
            conn.commit()
            print("âœ… ì¸ë±ìŠ¤ ë³µêµ¬ ì„±ê³µ")
        except Exception as rec_err:
            print(f"âŒ ì¸ë±ìŠ¤ ë³µêµ¬ ì‹¤íŒ¨: {rec_err}")
        finally:
            print("â˜‘ï¸ ì™¸ë˜í‚¤ ì œì•½ ì¡°ê±´ ë³µêµ¬")
            cursor.execute("SET FOREIGN_KEY_CHECKS = 1;")
            conn.commit()
            cursor.close()
            conn.close()
        print("âš ï¸ ì‚½ì… ì‹¤íŒ¨. ì¸ë±ìŠ¤ ë°±ì—… íŒŒì¼:", backup_file_name)
        return
    
    try:
        if indexes:
            print("ğŸ” ì¸ë±ìŠ¤ ë³µêµ¬ ì¤‘...")
            recreate_indexes(cursor, table_name, indexes)
            conn.commit()

        print("â˜‘ï¸ ì™¸ë˜í‚¤ ì œì•½ ì¡°ê±´ ë³µêµ¬")
        cursor.execute("SET FOREIGN_KEY_CHECKS = 1;")
        conn.commit()
    
    except mysql.connector.Error as post_err:
        print(f"âš ï¸ ì‚½ì…ì€ ì„±ê³µí–ˆì§€ë§Œ ì¸ë±ìŠ¤ ë³µêµ¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {post_err}")
        print("ë°±ì—… íŒŒì¼:", backup_file_name)

    cursor.close()
    conn.close()
    print(f"âœ… RDS upload complete. TABLE: {table_name}, {inserted_count} rows inserted.")

# row ë§ˆë‹¤ ê°œë³„ insert
def insert_rows_from_csv(filepath,table_name):
    # 1. DB ì—°ê²°
    conn = mysql.connector.connect(
        host=DB_HOST,
        user=DB_USER,
        password=DB_PASSWORD,
        database=DB_SCHEMA
    )
    cursor = conn.cursor()

    with open(filepath, newline='', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        headers = reader.fieldnames  # ì²« ì¤„ì˜ í•„ë“œëª… ë¦¬ìŠ¤íŠ¸
        column_str = ', '.join(f"`{col}`" for col in headers)
        placeholder_str = ', '.join(['%s'] * len(headers))

        sql = f"""
            INSERT INTO `{table_name}` ({column_str})
            VALUES ({placeholder_str})
        """

        inserted = 0
        for row in reader:
            values = [row[col].strip() if row[col] != "" else None for col in headers]
            cursor.execute(sql, values)
            inserted+=1

    conn.commit()
    print(f"âœ… INSERT ì™„ë£Œ: {inserted} rows inserted into `{table_name}`.")
    cursor.close()
    conn.close()


def insert_rows_from_csv_batch(filepath, table_name, batch_size=1000):
    # 1. DB ì—°ê²°
    conn = mysql.connector.connect(
        host=DB_HOST,
        user=DB_USER,
        password=DB_PASSWORD,
        database=DB_SCHEMA
    )
    cursor = conn.cursor()

    with open(filepath, newline='', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        headers = reader.fieldnames  # ì²« ì¤„ì˜ í•„ë“œëª… ë¦¬ìŠ¤íŠ¸
        column_str = ', '.join(f"`{col}`" for col in headers)
        placeholder_str = ', '.join(['%s'] * len(headers))

        sql = f"""
            INSERT INTO `{table_name}` ({column_str})
            VALUES ({placeholder_str})
        """

        batch = []
        total_inserted = 0

        for row in reader:
            values = [row[col].strip() if row[col] != "" else None for col in headers]
            batch.append(values)

            if len(batch) >= batch_size:
                cursor.executemany(sql, batch)
                total_inserted += len(batch)
                batch.clear()
                print("batch inserted")

        # ë§ˆì§€ë§‰ ë‚¨ì€ ë°ì´í„°
        if batch:
            cursor.executemany(sql, batch)
            total_inserted += len(batch)

    conn.commit()
    print(f"âœ… INSERT ì™„ë£Œ: {total_inserted} rows inserted into `{table_name}`.")
    cursor.close()
    conn.close()